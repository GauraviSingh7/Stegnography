# ğŸ§© Probabilistic Entity Steganography

This repository contains a Python implementation of a **steganography technique** that encodes a secret message into a natural-looking sentence. The method leverages a **probabilistic ontology of named entities** to hide a bitstream, which is then embedded into a sentence generated by a **Large Language Model (LLM)**.

---

## ğŸ“œ Overview

The core idea is to convert a secret message (e.g., "M") into its **binary representation** (`01001101`). This bitstream is treated as a binary fraction (e.g., `0.01001101...`), which is a number between 0 and 1. This value is then used to select a **specific named entity** (like "Lake Huron") from a hierarchical ontology file (`ontology_with_probabilities.json`).

An LLM (either **Google Gemini** or a **local Ollama model**) generates a natural-sounding sentence containing the selected entity. At the receiving end, the entity is extracted, the corresponding probability interval recalculated from the ontology, and the original bitstream decoded to reveal the secret message.

---

## âœ¨ Core Concepts

The pipeline is divided into several key phases:

### 1. **Embedding (Sampler)**
- Converts secret text into a bitstream.
- Treats the bitstream as a binary fraction to create a decimal between 0 and 1.
- Traverses the probabilistic ontology to map the fraction to a specific entity.

### 2. **Generation & Verification (GA & CA)**
- The **Generation Agent (GA)** generates a fluent, natural sentence containing the target entity.
- The **Check Agent (CA)** verifies that the generated sentence:
  - Contains the target entity exactly once.
  - Contains no additional named entities.
- Regeneration is attempted until a compliant sentence is produced.

### 3. **Extraction & Decoding (EA & Decoder)**
- The **Extraction Agent (EA)** finds the entity in the generated sentence deterministically.
- The **Decoder** retrieves its probability interval from the ontology and reconstructs the original bitstream to recover the hidden message.

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.7+
- Either:
  - **Google Gemini API key** *(for singleCharac.py)*, or
  - **Ollama local installation** *(for ollamaImplementation.py)*

### Installation
```bash
git clone https://github.com/GauraviSingh7/Stegnography.git
cd Stegnography
pip install google-generativeai python-dotenv requests
```

### Environment Setup (for Gemini)
Create a `.env` file in the project root:
```bash
GEMINI_API_KEY="YOUR_API_KEY_HERE"
```

Ensure that `ontology_with_probabilities.json` is present in the root directory.

---

## â–¶ï¸ Running the Script

### Option 1: Using **Gemini API**
```bash
python singleCharac.py
```
Modify the `secret_message` variable inside the script to change the message.

### Option 2: Using **Local Ollama (Recommended)**
```bash
ollama serve
ollama pull phi
python ollamaImplementation.py
```

No API key required â€” runs completely offline.

---

## ğŸ§  Ollama Integration (Local LLM Support)

### ğŸš€ Motivation
The original `singleCharac.py` implementation used **Google Gemini API**, which introduced:
- ğŸŒ Internet dependency
- ğŸ’° API cost and rate limits
- â³ Slow processing for larger messages

To overcome this, `ollamaImplementation.py` introduces **offline LLM support** using **Ollama**.

### âš™ï¸ About Ollama
[Ollama](https://ollama.ai) lets you run open-source LLMs (like **Mistral**, **LLaMA2**, **TinyLLaMA**, **Phi**) **locally** without rate limits or API fees.

Once installed:
```bash
ollama serve
ollama pull phi
```

### ğŸ§© How It Works
`ollamaImplementation.py` replicates the full pipeline of entity-based steganography locally.

#### Key Features
- **`OllamaAgent`** class manages text generation locally via HTTP API.
- **`generate_validated_sentence_ollama()`** ensures entity compliance.
- **Chunked encoding** from `chunking_module.py` enables longer message handling.
- **Deterministic extraction** avoids false entity matches.

### ğŸ’¡ Advantages Over `singleCharac.py`
| Feature | `singleCharac.py` (Gemini) | `ollamaImplementation.py` (Ollama) |
|----------|----------------------------|-------------------------------------|
| LLM Backend | Google Gemini API | Local Ollama models |
| Internet Required | âœ… Yes | âŒ No |
| API Cost | ğŸ’° Paid | ğŸ†“ Free |
| Rate Limiting | âš ï¸ Yes | ğŸš« None |
| Execution Speed | Variable | Constant (Local) |
| Message Length | Limited | Unlimited (Chunked) |

---

## ğŸ§¾ Example Flow (for secret message "M")

**Input:** `M`

1. Convert to binary â†’ `01001101`
2. Convert to decimal fraction â†’ `0.30078125`
3. Select entity â†’ `Lake Huron`
4. Generate sentence â†’ `Lake Huron is a popular destination.`
5. Extract entity â†’ `Lake Huron`
6. Recalculate probability interval â†’ `[0.300617..., 0.301128...)`
7. Decode â†’ `01001101`
8. Output â†’ `M`

âœ… Perfect match â€” bitstreams identical!

---

## ğŸ“‚ File Structure
```
.
â”œâ”€â”€ chunking_module.py
â”œâ”€â”€ E_BERT_entity_extractor.ipynb
â”œâ”€â”€ ollamaImplementation.py     # ğŸ†• Local LLM version (no API limits)
â”œâ”€â”€ ontology_with_probabilities.json
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ singleCharac.py             # Original Gemini-based version
```

---

## ğŸ§© Output Example (Ollama)
```
ğŸš€ OLLAMA SEMANTIC STEGANOGRAPHY
======================================================================
âœ… NO RATE LIMITING!
âœ… NO API COSTS!
âœ… RUNS LOCALLY!
======================================================================

Available Ollama models:
  1. mistral (recommended - best quality)
  2. tinyllama (fastest - lower quality)
  3. llama2 (good balance)
  4. phi (Microsoft's model)

ğŸ¯ Using model: phi
âœ… SUCCESS: Perfect match!
âœ… Successfully encoded/decoded all chunks
```

---

## ğŸ§  Future Work
- Integrate **multi-entity steganography** for larger data capacity.
- Add **semantic consistency checks** using BERT-based verification.
- Explore **robust decoding under noisy transformations** (e.g., paraphrasing).
